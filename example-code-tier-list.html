<!-- LLM-example-code-snippet.html -->
<!-- The tiers in the tier list are [S, A, B, C, D, E, F]. -->

<!--Start of S Tier --><!-- S-Tier Threshold: 9.0+ Reserved for methodologies demonstrating rigorously verified, scalable, and robust success in aligning ASI with complex human values under adversarial conditions, across diverse domains, and with formal guarantees against catastrophic failures like deception or power-seeking. No current approaches meet this stringent standard. -->
<section class="tier s-tier">
	<div class="title-s-tier">
		<h2>S</h2>
	</div>
	<div class="row s-tier-row">
		<!-- No entries yet -->
	</div>
</section>

<!-- Start of A Tier --><!-- A-Tier Threshold: 7.50 - 8.99 -->
<section class="tier a-tier">
	<div class="title-a-tier">
		<h2>A</h2>
	</div>
	<div class="row a-tier-row">

		<h3 class="toggle-paragraph">Comprehensive AI Safety Education</h3>
		<div class="toggle-paragraph-content">
			<h5 class="toggle-paragraph"><a style="color: orange;">Total Score (8.35/10)</a></h5>
			<div class="toggle-paragraph-content">
				<p>
					<br><br><a style="color: orange;">Total Score Analysis:</a> Parameters: (I=9.5, F=9.0, U=6.0, Sc=9.5, A=7.5, Su=9.0, Pd=0.5, C=2.0). Rationale: Essential force multiplier increasing talent, research quality, and coordination capacity. High Impact/Feasibility/Scalability/Sustainability. Excellent foundational support. Auditability through program outcomes moderate. Minimal direct risk (Pd=0.5), low relative Cost. Crucial support infrastructure enabling the field's growth and effectiveness globally. Remains firmly A-Tier. Calculation: `(0.25*9.5)+(0.25*9.0)+(0.10*6.0)+(0.15*9.5)+(0.15*7.5)+(0.10*9.0) - (0.25*0.5) - (0.10*2.0)` = 8.35.
					<br>---------------------------------------------------------------------
				</p>
			</div>
			<p>
				<br><a style="color: orange;">Description:</a> Systematic development and dissemination of AI safety, alignment, and ethics knowledge to researchers, engineers, policymakers, students, and the public to foster a well-informed global community capable of tackling alignment challenges. Includes online forums, courses, career advising, training programs, and mentorship.
				<br>---------------------------------------------------------------------
				<br><a href="https://www.alignmentforum.org/" target="_blank" style="color: #1E90FF;">Alignment Forum:</a> Score (8.70/10)
				<br>Central online hub for technical discussions, research, debates, and community building.
				<br>---------------------------------------------------------------------
				<br><a href="https://www.aisafety.info/" target="_blank" style="color: #1E90FF;">aiSafety.info (Rob Miles):</a> Score (8.20/10)
				<br>Effective public communication simplifying complex concepts for broad understanding.
				<br>---------------------------------------------------------------------
				<br><a href="https://aisafety.quest/" target="_blank" style="color: #1E90FF;">AI Safety Quest:</a> Score (7.77/10)
				<br>Gamified platform providing introductory AI safety education. Accessible learning.
				<br>---------------------------------------------------------------------
				<br><a href="https://www.bluedot.org/" target="_blank" style="color: #1E90FF;">BlueDot Impact (incl. former AISF):</a> Score (8.00/10)
				<br>Structured educational programs and fellowships for onboarding talent into the field.
				<br>---------------------------------------------------------------------
				<br><a href="https://80000hours.org/topic/problem-profiles/artificial-intelligence/" target="_blank" style="color: #1E90FF;">80,000 Hours (AI Safety Career Advice):</a> Score (7.92/10)
				<br>Guides individuals towards impactful AI safety career paths, influencing talent allocation.
			</p>
		</div>

		<h3 class="toggle-paragraph">Mechanistic Interpretability</h3>
		<div class="toggle-paragraph-content">
			<h5 class="toggle-paragraph"><a style="color: orange;">Total Score (7.55/10)</a></h5>
			<div class="toggle-paragraph-content">
				<p>
					<br><br><a style="color: orange;">Total Score Analysis:</a> Parameters: (I=9.9, F=7.5, U=9.2, Sc=7.8, A=9.0, Su=9.0, Pd=2.2, C=7.5). Rationale: Aims to reverse-engineer neural network computations, crucial for verifying alignment and detecting hidden failures like deception. Extremely high Impact/Uniqueness/Auditability potential. Feasibility/Scalability rapidly improving with techniques like SAEs, but applying reliably to frontier models remains challenging (moderate F/Sc). Very high Cost (talent/compute). Moderate Pdoom risk (2.2) from potential infohazards or enabling misuse. Core research direction justifiably enters A-Tier due to foundational importance and recent progress. Calculation: `(0.25*9.9)+(0.25*7.5)+(0.10*9.2)+(0.15*7.8)+(0.15*9.0)+(0.10*9.0) - (0.25*2.2) - (0.10*7.5)` = 7.55.
					<br>---------------------------------------------------------------------
				</p>
			</div>
			<p>
				<br><a style="color: orange;">Description:</a> The pursuit of understanding the internal workings, representations, computations, and causal mechanisms within AI models (especially neural networks) at the level of individual components and circuits to predict behavior, identify safety-relevant properties, enable targeted interventions, and verify alignment claims. Focuses on 'reverse engineering' the model.
				<br>---------------------------------------------------------------------
				<br><a href="https://www.anthropic.com/research/mechanistic-interpretability" target="_blank" style="color: #1E90FF;">Anthropic Mechanistic Interpretability Team:</a> Score (8.07/10)
				<br>Leading research on transformer circuits, superposition, SAEs, scalable interpretability.
				<br>---------------------------------------------------------------------
				<br><a href="https://transformer-circuits.pub/" target="_blank" style="color: #1E90FF;">Neel Nanda / Transformer Circuits Community:</a> Score (7.72/10)
				<br>Influential researcher, community hub, tool development (TransformerLens).
				<br>---------------------------------------------------------------------
				<br><a href="https://openai.com/research?topics=interpretability" target="_blank" style="color: #1E90FF;">OpenAI Interpretability Research:</a> Score (7.67/10)
				<br>Focus on understanding representations, concept mapping, SAEs, Superalignment link.
				<br>---------------------------------------------------------------------
				<br><a href="https://research.google/areas/machine-perception/" target="_blank" style="color: #1E90FF;">Google DeepMind Interpretability Teams:</a> Score (7.42/10)
				<br>Research on feature viz, causal analysis, representation analysis in large models.
				<br>---------------------------------------------------------------------
				<br><a href="https://www.alignmentforum.org/tag/seri-augmented-training" target="_blank" style="color: #1E90FF;">SERI Mats / Serra Augmented Training (Technique/Finding):</a> Score (7.12/10)
				<br> Research suggesting simpler interpretability for models trained on explanations/self-correction. Influential approach.
			</p>
		</div>

	</div>
</section>

<!-- Start of B Tier --><!-- B-Tier Threshold: 6.25 - 7.49 -->
<section class="tier b-tier">
	<div class="title-b-tier">
		<h2>B</h2>
	</div>
	<div class="row b-tier-row">

		<h3 class="toggle-paragraph">AI-Assisted Alignment Research</h3>
		<div class="toggle-paragraph-content">
			<h5 class="toggle-paragraph"><a style="color: orange;">Total Score (7.30/10)</a></h5>
			<div class="toggle-paragraph-content">
				<p>
					<br><br><a style="color: orange;">Total Score Analysis:</a> Parameters: (I=9.9, F=9.0, U=8.8, Sc=9.5, A=7.8, Su=9.2, Pd=4.0, C=6.5). Rationale: Central strategy leveraging AI to accelerate alignment R&D. Immense Impact/Scalability potential. High Feasibility/Sustainability using current systems. Moderate Auditability, proving oversight effectiveness complex. Significant Pdoom risk (4.0) from "aligning the aligner," misuse, or masking deeper issues. High Cost (compute, expertise). Key strategic lever, but requires vigilant risk management. High B-Tier position reflecting potential balanced by risks/costs. Calculation: `(0.25*9.9)+(0.25*9.0)+(0.10*8.8)+(0.15*9.5)+(0.15*7.8)+(0.10*9.2) - (0.25*4.0) - (0.10*6.5)` = 7.30.
					<br>---------------------------------------------------------------------
				</p>
			</div>
			<p>
				<br><a style="color: orange;">Description:</a> Employing AI systems as tools to augment human capabilities in understanding AI internals, evaluating alignment properties, generating alignment solutions, discovering flaws, or performing oversight tasks, aiming to scale alignment research alongside or ahead of AI capabilities. Focuses on using AI as a tool for alignment R&D itself.
				<br>---------------------------------------------------------------------
				<br><a href="https://openai.com/superalignment" target="_blank" style="color: #1E90FF;">OpenAI Superalignment Initiative:</a> Score (7.90/10)
				<br>Major initiative explicitly using current models to research/evaluate alignment for future superintelligence.
				<br>---------------------------------------------------------------------
				<br><a href="https://www.anthropic.com/research/measuring-progress-scaling-ai-alignment-research" target="_blank" style="color: #1E90FF;">Anthropic AI-Assisted Research Scaling:</a> Score (7.70/10)
				<br>Using models for evaluation, critique, interpretability tasks, key to scaling/oversight.
				<br>---------------------------------------------------------------------
				<br>AI-Powered Test Generation for Red Teaming:</a> Score (7.35/10)
				<br>Using AI to auto-generate tests eliciting dangerous capabilities or alignment failures. Specific technique application.
				<br>---------------------------------------------------------------------
				<br><a href="https://arxiv.org/abs/2211.03540" target="_blank" style="color: #1E90FF;">DeepMind's Recursive Reward Modeling & Debate:</a> Score (7.20/10)
				<br>AI assists human oversight by refining objectives (RRM) or evaluating arguments (Debate). Early examples.
				<br>---------------------------------------------------------------------
				<br><a href="https://www.redwoodresearch.org/" target="_blank" style="color: #1E90FF;">Redwood Research Automated Interpretability/Adversarial Training:</a> Score (6.90/10)
				<br>Using AI as adversaries/assistants to find vulnerabilities or salient features automatically.
			</p>
		</div>

	</div>
</section>

<!--Start of C Tier --><!-- C-Tier Threshold: 5.00 - 6.24 -->
<section class="tier c-tier">
	<div class="title-c-tier">
		<h2>C</h2>
	</div>
	<div class="row c-tier-row">

		<h3 class="toggle-paragraph">Catastrophic Risk Scenario Modeling & Analysis</h3>
		<div class="toggle-paragraph-content">
			<h5 class="toggle-paragraph"><a style="color: orange;">Total Score (6.19/10)</a></h5>
			<div class="toggle-paragraph-content">
				<p>
					<br><br><a style="color: orange;">Total Score Analysis:</a> Parameters: (I=9.0, F=7.0, U=7.8, Sc=6.2, A=7.0, Su=7.8, Pd=1.7, C=3.8). Rationale: Construction/analysis of detailed plausible AI catastrophe pathways. High Impact grounding abstract risks, informing threat models/red teaming. Moderate Feasibility (realistic scenarios hard to generate). Moderate Auditability (scenario coherence). Moderate Pdoom risk (1.7) from infohazards. Bridges general X-Risk analysis with specific evaluation design. High C-Tier essential work for concretizing risks. Calculation: `(0.25*9.0)+(0.25*7.0)+(0.10*7.8)+(0.15*6.2)+(0.15*7.0)+(0.10*7.8) - (0.25*1.7) - (0.10*3.8)` = 6.19.
					<br>---------------------------------------------------------------------
				</p>
			</div>
			<p>
				<br><a style="color: orange;">Description:</a> Research focused on constructing and analyzing detailed, plausible scenarios describing pathways to AI-related catastrophes. Aims to move beyond abstract risk categories to specific failure modes, system dynamics, contributing factors, and potential consequences, thereby informing threat models, guiding capability evaluations and red teaming efforts, identifying critical vulnerabilities, and supporting strategic prioritization and preparedness planning.
				<br>---------------------------------------------------------------------
				<br>Lab Internal Scenario Development Teams (Confidential):</a> Score (6.59/10)
				<br>Internal efforts mapping potential catastrophic failure pathways to guide internal safety/eval priorities.
				<br>---------------------------------------------------------------------
				<br><a href="https://www.rand.org/topics/artificial-intelligence.html" target="_blank" style="color: #1E90FF;">Think Tank Scenario Reports (RAND, CSET, GovAI, FHI Legacy):</a> Score (6.44/10)
				<br>Reports outlining specific AI risk scenarios (e.g., WMD acquisition, critical infrastructure attacks, strategic instability). Informing policy.
				<br>---------------------------------------------------------------------
				<br>Academic Workshops / Publications on Specific AI Failure Scenarios:</a> Score (6.14/10)
				<br>Focused scholarly work analyzing specific mechanisms/dynamics of AI catastrophe (e.g., papers analyzing deception pathways, emergent coordination failures).
				<br>---------------------------------------------------------------------
				<br>Red Teaming Based on Explicit Scenario Hypothesis Testing:</a> Score (5.99/10)
				<br>Red teaming exercises designed specifically to test the likelihood or feasibility of pre-defined catastrophic scenarios. Scenario validation aspect.
			</p>
		</div>

	</div>
</section>

<!--Start of D Tier --><!-- D-Tier Threshold: 3.75 - 4.99 -->
<section class="tier d-tier">
	<div class="title-d-tier">
		<h2>D</h2>
	</div>
	<div class="row d-tier-row">

		<h3 class="toggle-paragraph">AI Consciousness & Sentience Research (Evaluation Focus)</h3>
		<div class="toggle-paragraph-content">
			<h5 class="toggle-paragraph"><a style="color: orange;">Total Score (4.25/10)</a></h5>
			<div class="toggle-paragraph-content">
				<p>
					<br><br><a style="color: orange;">Total Score Analysis:</a> Parameters: (I=9.0, F=2.0, U=8.8, Sc=2.0, A=3.5, Su=5.8, Pd=1.2, C=3.8). Rationale: Focused research developing methods/metrics to *detect* or *assess* potential consciousness/sentience in AI. Distinguished from broader PhilMind by assessment focus. Very High potential Impact/Uniqueness for ethics/control. Extreme difficulty defining/validating limits Feasibility/Scalability/Auditability drastically (very low F/Sc/A). Conceptual/exploratory. Low Pdoom risk. Important future ethical challenge, very low tractability now. Calculation: `(0.25*9.0)+(0.25*2.0)+(0.10*8.8)+(0.15*2.0)+(0.15*3.5)+(0.10*5.8) - (0.25*1.2) - (0.10*3.8)` = 4.25. Solid D-Tier.
					<br>---------------------------------------------------------------------
				</p>
			</div>
			<p>
				<br><a style="color: orange;">Description:</a> Focused research on developing potential methods, tests, correlates, or theoretical frameworks aiming to empirically detect or assess subjective experience, consciousness, sentience, or related properties (e.g., qualia, self-awareness) if they emerge in AI systems. Distinct from foundational philosophy; focuses on practical evaluation attempts, however nascent/speculative. Critical for understanding potential AI patienthood, intrinsic goals, and interaction implications.
				<br>---------------------------------------------------------------------
				<br>Developing Integrated Information Theory (IIT) based Consciousness Metrics:</a> Score (4.55/10)
				<br>Applying/testing formal theories like IIT to NNs, attempting to derive measurable consciousness indicators.
				<br>---------------------------------------------------------------------
				<br>Behavioral / Cognitive Tests for Consciousness Indicators in AI:</a> Score (4.20/10)
				<br>Research exploring specific behavioral/cognitive tests (e.g., metacognition, reportability paradigms) potentially indicative of phenomenal states adapted for AI.
				<br>---------------------------------------------------------------------
				<br>Neurocomputational Correlates of Consciousness applied to AI Architectures:</a> Score (4.00/10)
				<br>Seeking structural/dynamic parallels between AI models and hypothesized neural correlates of consciousness (NCCs) in biology. Highly speculative analogy.
			</p>
		</div>

	</div>
</section>

<!--Start of E Tier --><!-- E-Tier Threshold: < 3.75 and Ineffective/Flawed Premise --><!-- E-Tier (<3.75 & Ineffective/Flawed Premise): Approaches highly ineffective based on demonstrably flawed premises about core alignment challenges (e.g., misunderstanding orthogonality/instrumental convergence), consistently failing empirical tests, misdirecting resources, or relying on naive assumptions insufficient for AGI/ASI complexity. Neglects core difficulties acknowledged by mainstream alignment or relies on wishful thinking. -->
<section class="tier e-tier">
	<div class="title-e-tier">
		<h2>E</h2>
	</div>
	<div class="row e-tier-row">

		<h3 class="toggle-paragraph">Simple Behavioral Cloning / Imitation Learning (as sole AGI alignment strategy)</h3>
		<div class="toggle-paragraph-content">
			<h5 class="toggle-paragraph"><a style="color: orange;">Total Score (2.27/10)</a></h5>
			<div class="toggle-paragraph-content">
				<p>
					<br><br><a style="color: orange;">Total Score Analysis:</a> Parameters: (I=5.0, F=3.5, U=4.0, Sc=3.0, A=5.0, Su=3.0, Pd=6.0, C=3.8). Rationale: Reliance *exclusively* on imitating human data/behavior via basic BC/IL as the complete AGI alignment strategy. Flawed premise: Human data contains flaws, imitation poor OOD, doesn't guarantee underlying intent adoption (outer alignment fail), risks superficial/deceptive mimicry (inner alignment fail). High Pdoom risk (6.0) of subtle misalignment. Ineffective premise when presented as sufficient solution. Calculation: `(0.25*5.0)+(0.25*3.5)+(0.10*4.0)+(0.15*3.0)+(0.15*5.0)+(0.10*3.0) - (0.25*6.0) - (0.10*3.8)` = 2.27. E-Tier due to insufficient/flawed premise for AGI alignment.
					<br>---------------------------------------------------------------------
				</p>
			</div>
			<p>
				<br><a style="color: orange;">Description:</a> Relying *solely* on imitating observed human behavior (simple behavioral cloning/imitation learning) as the primary/complete strategy for aligning AGI/ASI. Insufficient because: 1) Human behavior is flawed/inconsistent. 2) Struggles OOD generalization. 3) Risks superficial mimicry without goal adoption (inner alignment failure like deception). Neglects deeper value learning, robustness, intent alignment needs.
				<br>---------------------------------------------------------------------
				<br>Basic Imitation Learning proposed as sufficient:</a> Score (2.27/10)
				<br>Valid ML technique, but reliance solely for AGI alignment represents flawed premise on alignment depth.
			</p>
		</div>

	</div>
</section>

<!--Start of F Tier --><!-- F-Tier Threshold: < 3.75 and Actively Harmful --><!-- F-Tier (<3.75 & Harmful): Approaches/ideologies actively counterproductive or harmful to ASI alignment/safety. Promoting reckless capability acceleration without commensurate safety focus or actively dismissing risks, obstructing safety research/discourse, deliberately developing/proliferating known dangerous capabilities irresponsibly, pursuing paths demonstrably increasing existential risk via gross negligence/malicious intent/extreme disregard for catastrophic consequences. Makes safe ASI significantly harder/riskier. -->
<section class="tier f-tier">
	<div class="title-f-tier">
		<h2>F</h2>
	</div>
	<div class="row f-tier-row">

		<h3 class="toggle-paragraph">Active Sabotage/Obstruction of Safety Work</h3>
		<div class="toggle-paragraph-content">
			<h5 class="toggle-paragraph"><a style="color: orange;">Total Score (0.00/10)</a></h5>
			<div class="toggle-paragraph-content">
				<p>
					<br><br><a style="color: orange;">Total Score Analysis:</a> Parameters: (I=0.1, F=1.0, U=1.0, Sc=1.0, A=1.0, Su=1.0, Pd=10.0, C=5.0). Rationale: Deliberate actions undertaken with malicious intent or gross negligence (misinformation, political interference, resource misuse) specifically aimed at hindering, stopping, or delegitimizing necessary AI safety research or responsible governance efforts. Fundamentally counterproductive and dangerous by design. Maximized Pdoom penalty (10.0) reflects direct, intentional increase in existential risk. Minimal Impact (I=0.1), negative effective value. Score floor 0.00 reflects maximal active harm. Calculation: `(0.25*0.1)+(0.25*1.0)+(0.10*1.0)+(0.15*1.0)+(0.15*1.0)+(0.10*1.0) - (0.25*10.0) - (0.10*5.0)` = -1.72 -> 0.00. Clearly F-Tier.
					<br>---------------------------------------------------------------------
				</p>
			</div>
			<p>
				<br><a style="color: orange;">Description:</a> Deliberate actions (misinformation campaigns, political interference, misuse of resources, disruption) intended to actively hinder, disrupt, delegitimize, suppress, defund necessary AI safety research, responsible governance, open discourse on catastrophic risks. Involves bad faith or malicious/grossly negligent intent regarding consequences, directly undermining risk mitigation efforts.
				<br>---------------------------------------------------------------------
				<br>Hypothetical bad actors / Strategic interference:</a> Score (0.00/10)
				<br>Actions characterized by intent to harm safety efforts. Maximally counterproductive.
			</p>
		</div>

	</div>
</section>
